
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="./theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/font-awesome.min.css">







<meta name="author" content="Peter Wooldridge" />
<meta name="description" content="Derivation of parameter estimates for simple linear regression from first principles" />
<meta name="keywords" content="machine learning, linear models, R">

<meta property="og:site_name" content="Peter Wooldridge's Blog"/>
<meta property="og:title" content="Linear Regression from first principles"/>
<meta property="og:description" content="Derivation of parameter estimates for simple linear regression from first principles"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./linear_regression.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-09-12 21:46:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/peter-wooldridge.html">
<meta property="article:section" content="misc"/>
<meta property="article:tag" content="machine learning"/>
<meta property="article:tag" content="linear models"/>
<meta property="article:tag" content="R"/>
<meta property="og:image" content="/images/profile.JPG">

  <title>Peter Wooldridge's Blog &ndash; Linear Regression from first principles</title>

</head>
<body>
  <aside>
    <div>
      <a href=".">
        <img src="/images/profile.JPG" alt="Peter Wooldridge" title="Peter Wooldridge">
      </a>
      <h1><a href=".">Peter Wooldridge</a></h1>

<p>I'm a Machine Learning Engineer with Type 1 Diabetes. Most of the time I'll write about one or the other.</p>

      <ul class="social">
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href=".">    Home
</a>

      <a href="/pages/about.html">About</a>
      <a href="/categories">Categories</a>
      <a href="/tags">Tags</a>


    </nav>

<article class="single">
  <header>
      
    <h1 id="linear_regression">Linear Regression from first principles</h1>
    <p>
          Posted on Tue 12 September 2017 in <a href="./category/misc.html">misc</a>


    </p>
  </header>


  <div>
    <p>Linear regression allows one to model a dependent variable with the best straight line fit to a set of predictor variables. In the simplest scenario we have a single predictor variable. This is called simple linear regression.</p>
<p>As an example let's take the trees dataset provided by the datasets package in R. The data consists of measurements of girth, height and volume of 31 felled black cherry trees. Suppose we wish to try and predict the volume of a black cherry tree from its girth. To begin with we can plot the two variables using the following command:</p>
<div class="highlight"><pre><span></span><span class="nf">plot</span><span class="p">(</span><span class="n">trees</span><span class="o">$</span><span class="n">Girth</span><span class="p">,</span> <span class="n">trees</span><span class="o">$</span><span class="n">Volume</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Girth (inches)&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Volume (cubic feet)&quot;</span><span class="p">)</span>
</pre></div>
<div class="figure">
<img alt="" src="images/tree_girths.png" />
</div>
<p>The data certainly looks to have a linear relationship. A straight line equation in 2 dimensions can be defined by two variables namely the slope and intercept. Different choices for these parameters will produce different best fit lines as shown in the plot below:</p>
<div class="figure">
<img alt="" src="images/possible_linear_fits.png" />
</div>
<p>The left hand figure above shows linear fits with varying slopes and a fixed intercept. The right hand figure shows linear fits with varying intercepts and constant slopes. We want to find the combination of parameters for which our line best fits the data.</p>
<p>We'll return back to the trees example but suppose that in general we observe <span class="math">\(n\)</span> pairs of data points <span class="math">\((x_1, y_1), (x_2, y_2),\ldots, (x_n, y_n)\)</span> for <span class="math">\(i = 1,2,\ldots n\)</span>. We can define a linear relationship by writing each observation pair as follows:</p>
<p><span class="math">\(y_i = \beta_1 x_i + \beta_0 + \epsilon_i \qquad i = 1,2,\ldots n \tag{2}\label{2}\)</span></p>
<p><span class="math">\(\beta_0\)</span> and <span class="math">\(\beta_1\)</span> represent the slope and intercept respectively of our straight line fit. The <span class="math">\(\epsilon_i\)</span> are called residuals and represent the random variation of the <span class="math">\(y_i\)</span> around the straight line fit. Thought of another way each <span class="math">\(\epsilon_i\)</span> is the error between the true value of our response variable <span class="math">\(y_i\)</span> and the prediction given by our linear regression.</p>
<p>We wish to find an intercept/slope <span class="math">\((\beta_0, \beta_1)\)</span> combination that minimises the <span class="math">\(\epsilon_i\)</span> for <span class="math">\(i = 1,2,\ldots n\)</span> meaning that our prediction errors are small. The most popular method of achieving this is to compute the sum of the squares of the residuals for <span class="math">\(i = 1,2,\ldots n\)</span> and choose values of <span class="math">\(\beta_0\text{ and }\beta_1\)</span> that minimise this quanity. Mathematically we can write the residual sum of square (RSS) as follows:</p>
<p><span class="math">\(RSS = \sum_{i=1}^n \epsilon_i^2 \tag{3}\label{3}\)</span></p>
<p>rearranging <span class="math">\(\eqref{2}\)</span> to make <span class="math">\(\epsilon_i\)</span> the subject:</p>
<p><span class="math">\(\epsilon_i = y_i - \beta_1 x_i - \beta_0 \tag{4}\label{4}\)</span></p>
<p>substituting <span class="math">\(\eqref{4}\)</span> into <span class="math">\(\eqref{3}\)</span> we get:</p>
<p><span class="math">\(RSS = \sum_{i=1}^n (y_i - \beta_1 x_i - \beta_0)^2 \tag{5}\label{5}\)</span></p>
<p>To minimise <span class="math">\(\eqref{5}\)</span> we differentiate <span class="math">\(RSS\)</span> with respect to both <span class="math">\(\beta_0\)</span> and <span class="math">\(\beta_1\)</span> set the resulting quantity equal to zero and solve the two equations.</p>
<p>We can make use of the chain rule which states that for a composite function of the form:</p>
<div class="math">
\begin{align*}
&amp; F(x) = f(g(x)) \\
\end{align*}
</div>
<p>the deriviate is given by</p>
<p><span class="math">\(F'(x) = f'(g(x)) g'(x) \tag{6}\label{6}\)</span></p>
<p>Applying <span class="math">\(\eqref{6}\)</span> to <span class="math">\(\eqref{5}\)</span> we get:</p>
<p><span class="math">\(\frac{\partial RSS}{\partial \beta_0} = -2 \sum_{i=1}^n (y_i - \beta_1 x_i - \beta_0) \label{7}\tag{7}\)</span></p>
<p><span class="math">\(\frac{\partial RSS}{\partial \beta_1} = -2 \sum_{i=1}^n x_i (y_i - \beta_1 x_i - \beta_0) \label{8}\tag{8}\)</span></p>
<p>We set the above equations equal to zero and solve for <span class="math">\(\beta_0\)</span> and <span class="math">\(\beta_1\)</span> to find the minima. Starting with <span class="math">\(\eqref{7}\)</span>:</p>
<div class="math">
\begin{align*}
&amp; 0 = -2 \sum_{i=1}^n (y_i - \beta_1 x_i - \beta_0) \\
&amp; 0 = \sum_{i=1}^n (y_i - \beta_1 x_i - \beta_0) \\
&amp; 0 = \sum_{i=1}^n y_i - \beta_1 \sum_{i=0}^n x_i -n \beta_0 \\
&amp; n \beta_0 = \sum_{i=1}^n y_i - \beta_1 \sum_{i=1}^n x_i
\end{align*}
</div>
<p>making use of <span class="math">\(\bar{x} = \frac{\sum_{i=1}^n x_i}{n}\)</span> and <span class="math">\(\bar{y} = \frac{\sum_{i=1}^n y_i}{n}\)</span>:</p>
<div class="math">
\begin{align*}
&amp; \beta_0 = \bar{y} - \beta_1 \bar{x} \label{9}\tag{9} \\
\end{align*}
</div>
<p>Now we repeat the same process for <span class="math">\(\beta_{1}\)</span>:</p>
<div class="math">
\begin{align*}
&amp; 0 = -2 \sum_{i=1}^n x_i (y_i - \beta_1 x_i - \beta_0) \\
&amp; 0 = \sum_{i=1}^n x_i (y_i - \beta_1 x_i - \beta_0)
\end{align*}
</div>
<p>substituting in the value for <span class="math">\(\beta_0\)</span> obtained in <span class="math">\(\eqref{9}\)</span>:</p>
<div class="math">
\begin{align*}
&amp; 0 = -2 \sum_{i=1}^n x_i (y_i - \beta_1 x_i - \bar{y} + \beta_1 \bar{x}) \\
&amp; 0 = \sum_{i=1}^n x_i (y_i - \bar{y}) + \beta_1 \sum_{i=1}^n x_i (\bar{x} - x_i) \\
\\
&amp; \beta_1 = \frac{-\sum_{i=1}^n x_i(y_i - \bar{y})}{\sum_{i=1}^n x_i(\bar{x} - x_i)} \\
\\
&amp; \beta_1 = \frac{-\sum_{i=1}^n (x_i y_i - x_i \bar{y})}{\sum_{i=1}^n (x_i \bar{x} - x_i^2)} \\
\\
&amp; \beta_1 = \frac{\bar{y} \sum_{i=1}^n x_i - \sum_{i=1}^N x_i y_i}{\sum_{i=1}^n x_i - \sum_{i=1}^n x_i^2} \\
\\
&amp; \beta_1 = \frac{n \bar{x} \bar{y} - \sum_{i=1}^n x_i y_i}{n \bar{x}^2 - \sum_{i=1}^n x_i^2}
\end{align*}
</div>
<p>We have now derived least squares estimates for <span class="math">\(\beta_0\)</span> and <span class="math">\(\beta_1\)</span> using simple linear regression. Armed with these we can generate parameter estimates for the cherry tree regression:</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="nf">nrow</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>
<span class="n">x_bar</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">trees</span><span class="o">$</span><span class="n">Girth</span><span class="p">)</span>
<span class="n">y_bar</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">trees</span><span class="o">$</span><span class="n">Volume</span><span class="p">)</span>
<span class="n">xi_yi_sum</span> <span class="o">&lt;-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">trees</span><span class="o">$</span><span class="n">Girth</span> <span class="o">*</span> <span class="n">trees</span><span class="o">$</span><span class="n">Volume</span><span class="p">)</span>
<span class="n">xi_squared_sum</span> <span class="o">&lt;-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">trees</span><span class="o">$</span><span class="n">Girth</span> <span class="n">^</span> <span class="m">2</span><span class="p">)</span>

<span class="n">beta_1</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">x_bar</span> <span class="o">*</span> <span class="n">y_bar</span> <span class="o">-</span> <span class="n">xi_yi_sum</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">x_bar^2</span> <span class="o">-</span> <span class="n">xi_squared_sum</span><span class="p">)</span>
<span class="n">beta_0</span> <span class="o">&lt;-</span> <span class="n">y_bar</span> <span class="o">-</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">x_bar</span>

<span class="n">beta_1</span>
<span class="n">[1]</span> <span class="m">5.065856</span>
<span class="n">beta_0</span>
<span class="n">[1]</span> <span class="m">-36.94346</span>
</pre></div>
<p>Ofcourse R has a built in function (lm) to build a linear regression. We can use this function to compare the estimates we produced above:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">fit</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">trees</span><span class="o">$</span><span class="n">Volume</span><span class="o">~</span><span class="n">trees</span><span class="o">$</span><span class="n">Girth</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">fit</span><span class="o">$</span><span class="nf">coefficients</span>
<span class="p">(</span><span class="n">Intercept</span><span class="p">)</span> <span class="n">trees</span><span class="o">$</span><span class="n">Girth</span>
<span class="m">-36.943459</span>    <span class="m">5.065856</span>
</pre></div>
<p>which match our derived estimates.</p>
<script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        fonts: [['STIX', 'TeX']]," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/machine-learning.html">machine learning</a>
      <a href="./tag/linear-models.html">linear models</a>
      <a href="./tag/r.html">R</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Peter Wooldridge's Blog ",
  "url" : ".",
  "image": "/images/profile.JPG",
  "description": "Personal Blog"
}
</script>

</body>
</html>